\documentclass[a4paper,12pt]{article}
\usepackage[spanish]{babel}

% Packages - Plantilla
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{geometry}
\usepackage{fancyhdr}
\usepackage{setspace}
\usepackage{titlesec}  % For title formatting
\usepackage{tocloft}
\usepackage[colorlinks=true,linkcolor=black,citecolor=black,urlcolor=blue]{hyperref}
\usepackage{enumitem}
\setlist[itemize]{topsep=0pt, label=\textbullet, itemsep=0pt}
\usepackage{caption}
\captionsetup[table]{name=Tabla}

% Packages - User
\input{configuraciones/code}  % Configuración para snippets de código C/C++/CUDA

% Información de la práctica
\newcommand{\autor}{Pablo González Segarra}
\newcommand{\titulo}{Práctica 2}
\newcommand{\subtitulo}{Parte 1: Compilando Programas en C}
\newcommand{\fecha}{\today}
\newcommand{\titulacion}{Máster Universitario en Ingeniería de Computadores y Redes}

% Header and Footer
\pagestyle{fancy}
\fancyhf{}
\fancyhead[L]{\titulo}
\fancyhead[R]{\thepage}
\setlength{\headheight}{15pt}

% Other configurations
\geometry{margin=1in}
\setlength{\parskip}{0.5\baselineskip}
\setlength{\cftbeforesecskip}{0pt} % Espacio entre entradas del índice

% Tittle page
\input{tittle/tittle}

% Document
\begin{document}
% Title Page
\maketitle
\thispagestyle{empty}
\vfill

% Table of Contents
\setcounter{page}{1}  % Start counting from 1
\tableofcontents
\newpage

% 1
\section*{Ejercicio 1 y 2}\addcontentsline{toc}{section}{Ejercicio 1}
Debido a la similitud entre los ejercicios 1 y 2, se presentan juntos en esta sección.
A continuación se muestra el código modificado necesario para ejecutar el programa
en la GPU utilizando CUDA y comprobar que los resultados son correctos.
\lstinputlisting[
    language=CUDA, 
    style=codestyle, 
    caption={Código CUDA modificado para ejecutar en GPU}
    ]
    {../ej_1_y_2.cu}

% 3
\section*{Ejercicio 3}\addcontentsline{toc}{section}{Ejercicio 3}

El código modificado para poder obtener los tiempos de ejecución es el siguiente:

\lstinputlisting[
    language=CUDA, 
    style=codestyle, 
    caption={Código CUDA modificado para medir tiempos de ejecución}
    ]
    {../ej3.cu}

Se ha ejecutado 5 veces para evitar variaciones ajenas al rendimiento del código. El tiempo de 
La ejecución tiene en cuenta tanto la transferencia de datos entre host y device como la 
ejecución del kernel.

Resultado de las ejecuciones:
\begin{enumerate}[topsep=0pt,itemsep=0pt]
    \item 5887.4 ms
    \item 5864.1 ms
    \item 5918.6 ms
    \item 5856.9 ms
    \item 5868.1 ms
\end{enumerate}

Siendo el mejor tiempo 5856.9 ms.

% 4
\section*{Ejercicio 4}\addcontentsline{toc}{section}{Ejercicio 4}

Para comparar la aceleración obtenida al ejecutar el código en la GPU frente a la CPU,
primero se ha compilado y ejecutado la versión original en CPU, obteniendo un tiempo mejor de
992.6 ms.

Para obtener los tiempos de ejecución en GPU con distintas configuraciones, número de bloques
y hilos por bloque, se ha modificado el código del ejercicio 3 para variar estos parámetros. Todas las 
configuraciones se han ejecutado múltiples veces y se ha tomado el mejor tiempo de ejecución. En la 
tabla \ref{tab:tiempos-speedup} se muestran los resultados obtenidos.

\begin{table}[h!]
\centering
\begin{tabular}{lcc}
    \hline
	\textbf{Versión} & \textbf{Tiempo de ejecución (ms)} & \textbf{Speedup GPU vs CPU} \\
    \hline
    CPU (original) & 992.6 & 1.00 \\
    1 thread / block & 532.3 & 1.86 \\
    16 threads / block & 438.2 & 2.27 \\
    32 threads / block & 431.8 & 2.30 \\
    256 threads / block & 428.4 & 2.32 \\
    \hline
\end{tabular}
\caption{Tiempos de ejecución y speedup para distintas configuraciones de hilos/ bloque}
\label{tab:tiempos-speedup}
\end{table}

Como se puede observar, la paralelización en GPU ofrece una mejora significativa en el tiempo de
ejecución en comparación con la versión en CPU. El mejor rendimiento se obtiene con 256 hilos por bloque,
logrando un speedup de aproximadamente 2.32 veces respecto a la ejecución en CPU. Sin embargo, la 
mejora a partir de 16 hilos por bloque es marginal.

A continuación se muestra el código modificado para variar el número de bloques e hilos por bloque:
\lstinputlisting[
    language=CUDA, 
    style=codestyle, 
    caption={Código CUDA modificado para variar bloques e hilos por bloque}
    ]
    {../ej4.cu}


% 5
\section*{Ejercicio 5}\addcontentsline{toc}{section}{Ejercicio 5}

En este ejercicio se ha creado un kernel para la función \cppcode{void heavy_cpu(float* data, int n)}, el código.
se puede ver a continuación:

\begin{lstlisting}[
    language=CUDA, 
    style=codestyle, 
    caption={Código CUDA del kernel para la función heavy\_cpu}
    ]
__global__ void heavy_cpu(float* data, int n) {
    int i = blockIdx.x * blockDim.x + threadIdx.x;

    if (i < n) {
        float x = data[i];
        for (int j = 0; j < 10000; j++) {
            x = sinf(x) * 1.00001f + cosf(x) * 0.99999f;
        }
        data[i] = x;
    }
}
\end{lstlisting}

% 6
\section*{Ejercicio 6}\addcontentsline{toc}{section}{Ejercicio 6}

Para este ejercicio se ha usado la misma metodología que en los ejercicios anteriores, 
pero esta vez se ha medido solo el tiempo de ejecución del kernel, sin contar las 
transferencias de datos entre host y device.
Se ha ejecutado la función en CPU y GPU, obteniendo los siguientes tiempos:
\begin{itemize}[topsep=0pt,itemsep=0pt]
    \item CPU: 224,69 s
    \item GPU: 10,33 s
\end{itemize}

Suponiendo que la función \cppcode{heavy_cpu} contiene 5 operaciones de coma flotante
por iteración del bucle, y considerando que el bucle se ejecuta 10.000 veces por cada
elemento del array, podemos calcular el número total de operaciones de coma flotante
realizadas durante la ejecución de la función. Dado que el array tiene un tamaño de 
N = 1.000.000, el número total de operaciones de coma flotante es:
\[
\text{Total de operaciones} = N \times 10.000 \times 5 = 1.000.000 \times 10.000 \times 5 = 5 \
\times 10^{10} \text{ operaciones}\]

Con estos datos, podemos calcular el rendimiento en GFLOPS para ambas ejecuciones:
\begin{itemize}[topsep=0pt,itemsep=2pt]
    \item CPU: \(\frac{5 \times 10^{10} \text{ operaciones}}{224.69 \text{ s}} \approx 0.2226 \text{ GFLOPS}\)
    \item GPU: \(\frac{5 \times 10^{10} \text{ operaciones}}{10.33 \text{ s}} \approx 4.8385 \text{ GFLOPS}\)
\end{itemize}   

Estos resultados muestran una mejora significativa en el rendimiento al ejecutar
la función en la GPU en comparación con la CPU. Obteniendo una aceleración de 
aproximadamente 21.6 veces.

% 7
\section*{Ejercicio 7}\addcontentsline{toc}{section}{Ejercicio 7}

Cada hilo ejecuta el procesamiento de un elemento del array, es decir, obtiene su 
índice, lee \cppcode{data[i]}, aplica el bucle de 10.000 iteraciones con el grueso de la carga
computacional, y escribe el resultado de vuelta.

En cuanto al paralelismo, se podría mejorar ligeramente retocando algunos detalles
como los parámetros de threads per block o blocks, pero en general no es posible aumentar
significativamente el paralelismo porque cada iteración del bucle interno depende 
del valor anterior, por lo que esta dependencia impide paralelizar el bucle.


% 8 
\section*{Ejercicio 8}\addcontentsline{toc}{section}{Ejercicio 8}

En primer lugar, se ha medido el rendimiento ejecutando la función en la 
CPU. Para esto se ha usado N = 1000000, para permitir que el procesador 
termine en un tiempo razonable. Se ha obtenido un tiempo de ejecución
de 324 segundos, lo que da un rendimiento de 16,158 GFLOPS calculado de
la siguiente manera:
\[
\text{FLOPS} = \frac{2N^2}{\text{tiempo}} = \frac{2 \cdot (10^6)^2}{324} \approx 6,158 \text{ GFLOPS}
\]


La primera optimización que se ha realizado es el uso de la GPU, por lo que se 
ha implementado el kernel siguiente:

\begin{lstlisting}[
    language=CUDA, 
    style=codestyle, 
    caption={Código CUDA del kernel para la función vector\_pro}
    ]
__global__ void vector_pro(float *out, float *a, float *b, int n) {
    int i = blockIdx.x * blockDim.x + threadIdx.x;
    if (i < n) {
        out[i] = 0;
        for (int j= 0; j<n; j++){
            out[i] += a[i]*b[j];
        }
    }
}
\end{lstlisting}

Con la que se obtiene un tiempo de ejecución de 4389 ms, para 128 threads per block, 
lo que da un rendimiento de 455 GFLOPS, calculado de la siguiente manera:
\[
\text{FLOPS} = \frac{2N^2}{\text{tiempo}} = \frac{2 \cdot (10^6)^2}{4.389} \approx 455 \text{ GFLOPS}
\]
y una aceleración de 73.99x respecto a la CPU.

Analizando algebraicamente el kernel, se puede observar que este 
ejecuta la siguiente operación matemática:
\[
out[i] = \sum_{j=0}^{N-1} a[i] * b[j]
\]

esta operación se puede simplificar algebraicamente precalculando la suma de b,
lo que reduce la complejidad de la operación de $O(N^2)$ a $O(2N)$, de forma
que obtenemos la siguiente operación:
\[
out[i] = a[i] * \left(\sum_{j=0}^{N-1} b[j]\right)
\]  

Lo que permite implementar un kernel mucho más eficiente:
\begin{lstlisting}[
    language=CUDA, 
    style=codestyle, 
    caption={Código CUDA del kernel optimizado para la función vector\_pro}
    ]
__global__ void vector_pro(float *out, float *a, float sum_b, int n) {
    int i = blockIdx.x * blockDim.x + threadIdx.x;
    
    if (i < n) {
        out[i] = a[i] * sum_b;
    }
}
\end{lstlisting}

La suma de b se calcula previamente en la CPU y se pasa como parámetro al kernel. De
esta forma se obtiene un tiempo de ejecución de 0.9048 ms, para 256 threads per block,
lo que representa una aceleración de 359122.17x respecto a la CPU, lo que es realmente
sorprendente. Sin embargo al calcular el rendimiento en GFLOPS obtenemos un valor de 
22,9 GFLOPS, que es mucho menor que el obtenido en la versión anterior. El valor se 
ha calculado de la siguiente manera:
\[
\text{FLOPS} = \frac{2N}{\text{tiempo}} = \frac{2 \cdot (10^6)}{0.0009048} \approx 22,9 \text{ GFLOPS}
\]  

Mi suposición sobre el valor bajo de GFLOPS es que la GPU no está siendo
suficientemente utilizada, ya que la operación es muy simple y no requiere
muchos recursos computacionales (o lo estoy calculando mal, que siempre
es una opción :)). Por lo tanto, a pesar de la enorme reducción en tiempo de ejecución,
el número de operaciones por segundo no es tan alto como en la
versión anterior.

Con el objetivo de determinar de forma más precisa el rendimiento de la versión optimizada
se ha aumentado el tamaño de N, ya que con N=1.000.000 los tiempos son muy pequeños y pequeñas
variaciones pueden afectar significativamente los resultados. Por lo que se ha aumentado el 
valor de N a 100.000.000. Adicionalmente, es evidente que paralelizar 
la suma de b en la GPU puede aportar una mejora adicional, por lo que se ha implementado una version
que paraleliza la suma de b en la GPU.

Se han obtenido los siguientes resultados para 256 threads per block(la mejor configuración):
\begin{itemize}[topsep=0pt,itemsep=2pt]
    \item Versión secuencial: 133.5 ms
    \item Versión paralelizada (2 threads \footnotemark): 69.5 ms
\end{itemize}
\footnotetext{Se han usado 2 threads en la CPU para paralelizar la suma de b, ya que el equipo
solo dispone de 2 núcleos físicos. Se han realizado experimentos con 4, 8 y 12 threads, pero el rendimiento
empeoraba debido al overhead de gestión de los hilos.}

Vemos cómo la versión paralelizada ofrece una aceleración de 1.916x respecto a la versión
que realiza la suma de b de forma secuencial en la CPU. Esto hace pensar que el cuello de 
botella en la versión optimizada es la suma de b, por lo que se hizo un análisis más detallado
de los tiempos de ejecución, cuyos resultados se muestran a continuación:

\begin{table}[h!]
\centering
\begin{tabular}{lcc}
    \hline
    \textbf{Fase} & \textbf{Tiempo (ms)} & \textbf{Porcentaje del total} \\
    \hline
    Suma paralela (CPU) & 69.15 & 95.2\% \\
    Ejecución kernel (GPU) & 3.50 & 4.8\% \\
    \hline
    \textbf{Total} & 72.66 & 100\% \\
    \hline
\end{tabular}
\caption{Desglose de tiempos de ejecución para N=100.000.000, suma paralela en CPU y kernel en GPU.}
\label{tab:desglose-tiempos-ej8}
\end{table}
 
De todos los resultados obtenidos, es trivial observar que la suma de b es el cuello de botella y 
que paralelizarla en la CPU aporta una mejora significativa. Por lo que se podría intentar paralelizar
la suma de b en la GPU para intentar mejorar aún más el rendimiento.

Para eso se ha implementado el siguiente código:

\lstinputlisting[
    language=CUDA, 
    style=codestyle, 
    caption={Código CUDA para paralelizar la suma de b en GPU}
    ]
    {../ej8_sum_gpu.cu}

Como en los otros casos, se ha probado un amplio rango de configuraciones, obteniendo el mejor resultado
con 2048 bloques de sumas parciales y 256 threads per block, lo que nos da 191 elementos por hilo. Se han obtenidos
los siguientes resultados:

\begin{table}[h!]
\centering
\begin{tabular}{lcc}
    \hline
    \textbf{Configuración} & \textbf{Tiempo (ms)} & \textbf{GFLOPS} \\
    \hline
    2048 bloques, 256 threads/block, 191 elem/thread & 8.645 & 23,168 \\
    \hline
    \textbf{Speedup} & \textbf{Respecto a} & \textbf{Factor} \\
    \hline
    Suma secuencial en CPU & 133.53 ms & 15.44x \\
    Suma paralelizada en CPU & 69.15 ms & 8.00x \\
    \hline
\end{tabular}
\caption{Mejor configuración para la suma en GPU con atomicAdd, N=100.000.000}
\label{tab:mejor-suma-gpu}
\end{table}
            
A pesar de que las ganancias son significativas, especialmente respecto al kernel original,
aún hay margen de mejora, pero estas se escapan de mis capacidades actuales.

\cleardoublepage
\pagestyle{plain}
% Bibliography (if required)
% \bibliographystyle{ieeetr}
% \bibliography{references.bib}  % Add a .bib file if you have references

\end{document}
