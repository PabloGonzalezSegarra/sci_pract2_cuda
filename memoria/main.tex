\documentclass[a4paper,12pt]{article}
\usepackage[spanish]{babel}

% Packages - Plantilla
\usepackage{graphicx}
\usepackage{amsmath}
\usepackage{geometry}
\usepackage{fancyhdr}
\usepackage{setspace}
\usepackage{titlesec}  % For title formatting
\usepackage{tocloft}
\usepackage[colorlinks=true,linkcolor=black,citecolor=black,urlcolor=blue]{hyperref}
\usepackage{enumitem}
\setlist[itemize]{topsep=0pt, label=\textbullet, itemsep=0pt}
\usepackage{caption}
\captionsetup[table]{name=Tabla}

% Packages - User
\input{configuraciones/code}  % Configuración para snippets de código C/C++/CUDA

% Información de la práctica
\newcommand{\autor}{Pablo González Segarra}
\newcommand{\titulo}{Práctica 2}
\newcommand{\subtitulo}{Parte 1: Compilando Programas en C}
\newcommand{\fecha}{\today}
\newcommand{\titulacion}{Máster Universitario en Ingeniería de Computadores y Redes}

% Header and Footer
\pagestyle{fancy}
\fancyhf{}
\fancyhead[L]{\titulo}
\fancyhead[R]{\thepage}
\setlength{\headheight}{15pt}

% Other configurations
\geometry{margin=1in}
\setlength{\parskip}{0.5\baselineskip}
\setlength{\cftbeforesecskip}{0pt} % Espacio entre entradas del índice

% Tittle page
\input{tittle/tittle}

% Document
\begin{document}
% Title Page
\maketitle
\thispagestyle{empty}
\vfill

% Table of Contents
\setcounter{page}{1}  % Start counting from 1
\tableofcontents
\newpage

% 1
\section*{Ejercicio 1 y 2}\addcontentsline{toc}{section}{Ejercicio 1}
Debido a la similitud entre los ejercicios 1 y 2, se presentan juntos en esta sección.
A continuación se muestra el código modificado necesario para ejecutar el programa
en la GPU utilizando CUDA y comprobar que los resultados son correctos.
\lstinputlisting[
    language=CUDA, 
    style=codestyle, 
    caption={Código CUDA modificado para ejecutar en GPU}
    ]
    {../ej_1_y_2.cu}

% 3
\section*{Ejercicio 3}\addcontentsline{toc}{section}{Ejercicio 3}

El código modificado para poder obtener los tiempos de ejecución es el siguiente:

\lstinputlisting[
    language=CUDA, 
    style=codestyle, 
    caption={Código CUDA modificado para medir tiempos de ejecución}
    ]
    {../ej3.cu}

Se ha ejecutado 5 veces para evitar variaciones ajenas al rendimiento del código. El tiempo de 
La ejecución tiene en cuenta tanto la transferencia de datos entre host y device como la 
ejecución del kernel.

Resultado de las ejecuciones:
\begin{enumerate}[topsep=0pt,itemsep=0pt]
    \item 5887.4 ms
    \item 5864.1 ms
    \item 5918.6 ms
    \item 5856.9 ms
    \item 5868.1 ms
\end{enumerate}

Siendo el mejor tiempo 5856.9 ms.

% 4
\section*{Ejercicio 4}\addcontentsline{toc}{section}{Ejercicio 4}

Para comparar la aceleracion obtenida al ejecutar el código en la GPU frente a la CPU,
primero se ha compilado y ejecutado la version original en CPU, obteniendo un tiempo mejor de
992.6 ms.

Para obtener los tiempos de ejecución en GPU con distintas configuraciones numero de bloques
y hilos por bloque, se ha modificado el código del ejercicio 3 para variar estos parámetros. Todas las 
configuraciones se han ejecutado multiples veces y se ha tomado el mejor tiempo de ejecución. En la 
tabla \ref{tab:tiempos-speedup} se muestran los resultados obtenidos.

\begin{table}[h!]
\centering
\begin{tabular}{lcc}
    \hline
	\textbf{Versión} & \textbf{Tiempo de ejecución (ms)} & \textbf{Speedup GPU vs CPU} \\
    \hline
    CPU (original) & 992.6 & 1.00 \\
    1 thread / block & 532.3 & 1.86 \\
    16 threads / block & 438.2 & 2.27 \\
    32 threads / block & 431.8 & 2.30 \\
    256 threads / block & 428.4 & 2.32 \\
    \hline
\end{tabular}
\caption{Tiempos de ejecución y speedup para distintas configuraciones de hilos/ bloque}
\label{tab:tiempos-speedup}
\end{table}

Como se puede observar la parelelización en GPU ofrece una mejora significativa en el tiempo de
ejecución en comparación con la versión en CPU. El mejor rendimiento se obtiene con 256 hilos por bloque,
logrando un speedup de aproximadamente 2.32 veces respecto a la ejecución en CPU. Sin embargo, la 
mejora a partir de 16 hilos por bloque es marginal.

A continuación se muestra el código modificado para variar el número de bloques e hilos por bloque:
\lstinputlisting[
    language=CUDA, 
    style=codestyle, 
    caption={Código CUDA modificado para variar bloques e hilos por bloque}
    ]
    {../ej4.cu}


% 5
\section*{Ejercicio 5}\addcontentsline{toc}{section}{Ejercicio 5}

En este ejercicio se ha creado un kernel para la funcion \cppcode{void heavy_cpu(float* data, int n)}, el codigo
se puede ver a continuacion:

\begin{lstlisting}[
    language=CUDA, 
    style=codestyle, 
    caption={Código CUDA del kernel para la función heavy\_cpu}
    ]
__global__ void heavy_cpu(float* data, int n) {
    int i = blockIdx.x * blockDim.x + threadIdx.x;

    if (i < n) {
        float x = data[i];
        for (int j = 0; j < 10000; j++) {
            x = sinf(x) * 1.00001f + cosf(x) * 0.99999f;
        }
        data[i] = x;
    }
}
\end{lstlisting}

% 6
\section*{Ejercicio 6}\addcontentsline{toc}{section}{Ejercicio 6}

Para este ejercicio se ha usado la misma metodologia que en los ejercicios anteriores, 
pero esta vez se ha medido solo el tiempo de ejecucion del kernel, sin contar las 
transferencias de datos entre host y device.
Se ha ejecutado la funcion en CPU y GPU, obteniendo los siguientes tiempos:
\begin{itemize}[topsep=0pt,itemsep=0pt]
    \item CPU: 224,69 s
    \item GPU: 10,33 s
\end{itemize}

Suponiendo que la funcion \cppcode{heavy_cpu} contiene 5 operaciones de coma flotante
por iteracion del bucle, y considerando que el bucle se ejecuta 10.000 veces por cada
elemento del array, podemos calcular el número total de operaciones de coma flotante
realizadas durante la ejecución de la función. Dado que el array tiene un tamaño de 
N = 1.000.000, el número total de operaciones de coma flotante es:
\[
\text{Total de operaciones} = N \times 10.000 \times 5 = 1.000.000 \times 10.000 \times 5 = 5 \
\times 10^{10} \text{ operaciones}\]

Con estos datos, podemos calcular el rendimiento en GFLOPS para ambas ejecuciones:
\begin{itemize}[topsep=0pt,itemsep=2pt]
    \item CPU: \(\frac{5 \times 10^{10} \text{ operaciones}}{224.69 \text{ s}} \approx 0.2226 \text{ GFLOPS}\)
    \item GPU: \(\frac{5 \times 10^{10} \text{ operaciones}}{10.33 \text{ s}} \approx 4.8385 \text{ GFLOPS}\)
\end{itemize}   

Estos resultados muestran una mejora significativa en el rendimiento al ejecutar
la funcion en la GPU en comparación con la CPU. Obteniendo una aceleracion de 
aproximadamente 21.6 veces.

% 7
\section*{Ejercicio 7}\addcontentsline{toc}{section}{Ejercicio 7}

Cada hilo ejecuta el procesamiento de un elemento del array, es decir, obtine su 
índice, lee \cppcode{data[i]}, aplica el bucle de 10.000 iteraciones con el grueso de la carga
computacional, y escribe el resultado de vuelta.

En cuanto al paralelismo, se podria mejorar ligeramente retocando algunos detalles
como los parametros de threads per block o blocks, pero en general no es posible aumentar
significativmaente el paralelismo porque cada iteracion del bucle interno depende 
del valor anterior, por lo que esta dependencia impide paralelizar el bucle.


% 8 
\section*{Ejercicio 8}\addcontentsline{toc}{section}{Ejercicio 8}

\cleardoublepage
\pagestyle{plain}
% Bibliography (if required)
% \bibliographystyle{ieeetr}
% \bibliography{references.bib}  % Add a .bib file if you have references

\end{document}
